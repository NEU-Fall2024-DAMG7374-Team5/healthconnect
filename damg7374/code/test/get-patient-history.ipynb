{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENTID = '495425e0-b16b-0baa-f926-0dfbe9870673'\n",
    "PATIENTID = PATIENTID.upper()\n",
    "print(PATIENTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_url = \"bolt://localhost:7687\" \n",
    "neo4j_user = \"neo4j\" \n",
    "neo4j_password = \"***\"\n",
    "neo4j_db = \"poc\"\n",
    "SOURCE_DATASET_LOCATION = \"/Users/amk/#Datasets/synthea/csv/synthea50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "        .master('spark://Adityas-MacBook-Pro.local:7077')\n",
    "        .appName('HealthConnect Test')\n",
    "        .config('spark.ui.port', '4051')\n",
    "        .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "        .config('spark.jars', '/Users/amk/Github/healthconnect/damg7374/code/utilities/neo4j-spark-connector.jar')\n",
    "        .config(\"neo4j.url\", neo4j_url)\n",
    "        .config(\"neo4j.authentication.type\", \"basic\")\n",
    "        .config(\"neo4j.authentication.basic.username\", neo4j_user)\n",
    "        .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "        .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "        .config(\"neo4j.database\", neo4j_db)\n",
    "        .config('spark.executor.instances', 1)\n",
    "        .config('spark.executor.cores', 1) \n",
    "        .config('spark.executor.memory', '1G') \n",
    "        .config('spark.cores.max', 1) \n",
    "        .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"patients.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"patients.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(Id) = \"{PATIENTID}\"')\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"encounters.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"encounters.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(PATIENT) = \"{PATIENTID}\"').orderBy('Id')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n",
    "\n",
    "a = df.filter(f'upper(PATIENT) = \"{PATIENTID}\"').orderBy('Id').selectExpr('Id as ENCOUNTER','ORGANIZATION', 'PROVIDER', 'PAYER')\n",
    "b = a.toPandas().to_dict(orient='list')\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"organizations.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "c = b['ORGANIZATION']\n",
    "SOURCE_DATASET_FILENAME = \"organizations.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(col(\"Id\").isin(c))\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"providers.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "c = b['PROVIDER']\n",
    "SOURCE_DATASET_FILENAME = \"providers.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(col(\"Id\").isin(c))\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"payers.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "c = b['PAYER']\n",
    "SOURCE_DATASET_FILENAME = \"payers.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(col(\"Id\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"claims.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"claims.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(PATIENTID) = \"{PATIENTID}\"').orderBy('Id')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n",
    "\n",
    "x = df.filter(f'upper(PATIENTID) = \"{PATIENTID}\"').orderBy('Id').selectExpr('Id as CLAIM')\n",
    "y = x.toPandas().to_dict(orient='list')\n",
    "b.update(y)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"claims_transactions.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"claims_transactions.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(PATIENTID) = \"{PATIENTID}\"').orderBy('CLAIMID', 'CHARGEID')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n",
    "\n",
    "x = df.filter(f'upper(PATIENTID) = \"{PATIENTID}\"').orderBy('Id').selectExpr('Id as CLAIM_TRANSACTION')\n",
    "y = x.toPandas().to_dict(orient='list')\n",
    "b.update(y)\n",
    "# b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"allergies.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"allergies.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"careplans.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"careplans.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"conditions.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"conditions.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"devices.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"devices.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"imaging_studies.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"imaging_studies.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"immunizations.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"immunizations.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"medications.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"medications.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"observations.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"observations.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"procedures.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"procedures.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"supplies.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"supplies.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"payer_transitions.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"payer_transitions.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = [PATIENTID.lower()]\n",
    "a = df.filter(col(\"PATIENT\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['allergies.csv', 'conditions.csv', 'observations.csv', 'procedures.csv', 'medications.csv', 'immunizations.csv', 'devices.csv', 'supplies.csv', 'careplans.csv', 'imaging_studies.csv']\n",
    "\n",
    "SOURCE_DATASET_LOCATION = '/Users/amk/#Datasets/synthea/csv/synthea50'\n",
    "for i in a:\n",
    "    SOURCE_DATASET_FILENAME = i\n",
    "    df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "    df.select('ENCOUNTER').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Find Common ENCOUNTERs\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# List of CSV files\n",
    "files = [\n",
    "    'allergies.csv', 'conditions.csv', 'observations.csv', 'procedures.csv',\n",
    "    'medications.csv', 'immunizations.csv', 'devices.csv', 'supplies.csv',\n",
    "    'careplans.csv', 'imaging_studies.csv'\n",
    "]\n",
    "\n",
    "SOURCE_DATASET_LOCATION = '/Users/amk/#Datasets/synthea/csv/synthea50'\n",
    "\n",
    "# Read all files and select only the ENCOUNTER column\n",
    "dfs = [spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{file}\", header=True, inferSchema=True).select(\"ENCOUNTER\").distinct() for file in files]\n",
    "\n",
    "# Find common ENCOUNTER values across all files using an inner join\n",
    "common_encounters = reduce(lambda df1, df2: df1.join(df2, on=\"ENCOUNTER\", how=\"inner\"), dfs)\n",
    "\n",
    "# Show the common ENCOUNTER values\n",
    "common_encounters.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, count\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Find Most Frequent ENCOUNTER\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# List of CSV files\n",
    "files = [\n",
    "    'allergies.csv', 'conditions.csv', 'observations.csv', 'procedures.csv',\n",
    "    'medications.csv', 'immunizations.csv', 'devices.csv', 'supplies.csv',\n",
    "    'careplans.csv', 'imaging_studies.csv'\n",
    "]\n",
    "\n",
    "# Add a file identifier and union all dataframes\n",
    "dfs_with_file_ids = [\n",
    "    spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{file}\", header=True)\n",
    "    .select(\"ENCOUNTER\")\n",
    "    .distinct()\n",
    "    .withColumn(\"file\", lit(file)) for file in files\n",
    "]\n",
    "\n",
    "# Union all DataFrames\n",
    "union_df = reduce(lambda df1, df2: df1.union(df2), dfs_with_file_ids)\n",
    "\n",
    "# Count occurrences of each ENCOUNTER across files\n",
    "encounter_counts = union_df.groupBy(\"ENCOUNTER\").agg(count(\"file\").alias(\"file_count\"))\n",
    "\n",
    "# Find the ENCOUNTER with the maximum count\n",
    "most_common_encounter = encounter_counts.orderBy(col(\"file_count\").desc())\n",
    "\n",
    "# Show the result\n",
    "most_common_encounter.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
