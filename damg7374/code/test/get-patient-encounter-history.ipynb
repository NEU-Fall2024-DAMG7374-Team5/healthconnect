{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENTID = '751E1F62-93C3-8048-7F77-48415A978963'\n",
    "PATIENTID = PATIENTID.upper()\n",
    "print(PATIENTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_url = \"bolt://localhost:7687\" \n",
    "neo4j_user = \"neo4j\" \n",
    "neo4j_password = \"***\"\n",
    "neo4j_db = \"poc\"\n",
    "SOURCE_DATASET_LOCATION = \"/Users/amk/#Datasets/synthea/csv/synthea50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "        .master('spark://Adityas-MacBook-Pro.local:7077')\n",
    "        .appName('HealthConnect Test')\n",
    "        .config('spark.ui.port', '4051')\n",
    "        .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "        .config('spark.jars', '/Users/amk/Github/healthconnect/damg7374/code/utilities/neo4j-spark-connector.jar')\n",
    "        .config(\"neo4j.url\", neo4j_url)\n",
    "        .config(\"neo4j.authentication.type\", \"basic\")\n",
    "        .config(\"neo4j.authentication.basic.username\", neo4j_user)\n",
    "        .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "        .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "        .config(\"neo4j.database\", neo4j_db)\n",
    "        .config('spark.executor.instances', 1)\n",
    "        .config('spark.executor.cores', 1) \n",
    "        .config('spark.executor.memory', '1G') \n",
    "        .config('spark.cores.max', 1) \n",
    "        .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"patients.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"patients.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(Id) = \"{PATIENTID}\"')\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"encounters.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"encounters.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(PATIENT) = \"{PATIENTID}\"').orderBy('Id')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Encounter ID\n",
    "ENCOUNTERID = 'B41D26FD-733E-7773-0115-A9E5C0451FBC'\n",
    "ENCOUNTERID = ENCOUNTERID.upper()\n",
    "print(ENCOUNTERID)\n",
    "\n",
    "a = df.filter(f'upper(Id) = \"{ENCOUNTERID}\"')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n",
    "\n",
    "a = a = df.filter(f'upper(Id) = \"{ENCOUNTERID}\"').orderBy('Id').selectExpr('Id as ENCOUNTER','ORGANIZATION', 'PROVIDER', 'PAYER')\n",
    "b = a.toPandas().to_dict(orient='list')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"organizations.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "c = b['ORGANIZATION']\n",
    "SOURCE_DATASET_FILENAME = \"organizations.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(col(\"Id\").isin(c))\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"providers.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "c = b['PROVIDER']\n",
    "SOURCE_DATASET_FILENAME = \"providers.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(col(\"Id\").isin(c))\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"payers.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "c = b['PAYER']\n",
    "SOURCE_DATASET_FILENAME = \"payers.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(col(\"Id\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"claims.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"claims.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(APPOINTMENTID) = \"{ENCOUNTERID}\"').orderBy('Id')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Encounter ID\n",
    "CLAIMID = '076a15e9-1da5-ada1-699d-aef0b51e534b'\n",
    "CLAIMID = CLAIMID.upper()\n",
    "print(CLAIMID)\n",
    "\n",
    "x = df.filter(f'upper(Id) = \"{CLAIMID}\"').orderBy('Id').selectExpr('Id as CLAIM')\n",
    "y = x.toPandas().to_dict(orient='list')\n",
    "b.update(y)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"claims_transactions.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "CLAIMID = b['CLAIM'][0]\n",
    "SOURCE_DATASET_FILENAME = \"claims_transactions.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "a = df.filter(f'upper(CLAIMID) = upper(\"{CLAIMID}\")').orderBy('CLAIMID', 'CHARGEID')\n",
    "\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")\n",
    "\n",
    "x = df.filter(f'upper(CLAIMID) = upper(\"{CLAIMID}\")').orderBy('CLAIMID', 'CHARGEID').selectExpr('ID as CLAIM_TRANSACTION')\n",
    "y = x.toPandas().to_dict(orient='list')\n",
    "b.update(y)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"allergies.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"allergies.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"careplans.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"careplans.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"conditions.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"conditions.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"devices.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"devices.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"imaging_studies.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"imaging_studies.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"immunizations.csv\")\n",
    "print(\"================\")\n",
    "\n",
    "SOURCE_DATASET_FILENAME = \"immunizations.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"medications.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"medications.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"observations.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"observations.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"procedures.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"procedures.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"supplies.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"supplies.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = b['ENCOUNTER']\n",
    "a = df.filter(col(\"ENCOUNTER\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================\")\n",
    "print(\"payer_transitions.csv\")\n",
    "print(\"================\")\n",
    "SOURCE_DATASET_FILENAME = \"payer_transitions.csv\"\n",
    "df = (spark.read.csv(f\"{SOURCE_DATASET_LOCATION}/{SOURCE_DATASET_FILENAME}\", header=True, inferSchema=True))\n",
    "c = [PATIENTID.lower()]\n",
    "a = df.filter(col(\"PATIENT\").isin(c))\n",
    "a.show(a.count(), truncate=False)\n",
    "print(f\"Record count : {a.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
